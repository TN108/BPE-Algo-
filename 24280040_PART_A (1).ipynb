{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "525d61ef",
      "metadata": {
        "id": "525d61ef"
      },
      "source": [
        "# PA1 Part A: Byte Pair Encoding (BPE)\n",
        "\n",
        "## Overview\n",
        "In this assignment, you will implement the **Byte Pair Encoding (BPE)** algorithm **from scratch** using Python.  \n",
        "BPE is a widely used **subword tokenization algorithm** that forms the foundation of tokenizers used in modern language models such as GPT and BERT.\n",
        "\n",
        "Rather than relying on existing tokenizer libraries, you will build the algorithm step by step to understand:\n",
        "- How subword vocabularies are learned\n",
        "- How frequent character patterns are merged\n",
        "- How unseen or rare words are handled during tokenization\n",
        "\n",
        "By the end of this assignment, you will have a fully working BPE tokenizer that can be trained on a corpus and applied to new text.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Byte Pair Encoding?\n",
        "Byte Pair Encoding is an **unsupervised, frequency-based tokenization algorithm**.  \n",
        "It starts with individual characters as tokens and repeatedly merges the **most frequent adjacent token pairs** in the corpus. Over time, this process builds a vocabulary of meaningful subword units.\n",
        "\n",
        "BPE allows language models to:\n",
        "- Limit vocabulary size\n",
        "- Handle rare and unseen words\n",
        "- Capture common prefixes, suffixes, and roots\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Reading\n",
        "If you would like a more detailed explanation of the algorithm and worked examples, refer to the following resource:\n",
        "\n",
        "üîó **GeeksforGeeks ‚Äì Byte Pair Encoding (BPE) in NLP**  \n",
        "https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35069dc8",
      "metadata": {
        "id": "35069dc8"
      },
      "source": [
        "<div style=\"color:red\">\n",
        "\n",
        "## Rules & Constraints\n",
        "\n",
        "Please read the following rules carefully before starting the assignment.\n",
        "\n",
        "### ‚ùå Disallowed\n",
        "- You **may NOT** use any existing tokenization libraries  \n",
        "  (e.g., HuggingFace tokenizers, SentencePiece, GPT tokenizers, etc.)\n",
        "- You **may NOT** use pre-implemented BPE code from external sources\n",
        "\n",
        "### ‚úÖ Allowed\n",
        "- Standard Python libraries such as:\n",
        "  - `collections`\n",
        "  - `re`\n",
        "  - `math`\n",
        "- Pure Python data structures (lists, dictionaries, tuples)\n",
        "\n",
        "### üìå Implementation Requirements\n",
        "- All parts of the **BPE algorithm must be implemented from scratch**\n",
        "- You must follow the function signatures provided in the notebook\n",
        "- Your code should be **clear, modular, and well-commented**\n",
        "- Do **not** modify the provided test cells (if any)\n",
        "\n",
        "### ‚ö†Ô∏è Academic Integrity\n",
        "- You must write and submit **your own code**\n",
        "- Copying implementations from online sources will be considered plagiarism\n",
        "\n",
        "\n",
        "## Instructions\n",
        "- <font color=\"red\">Proceed cell by cell and complete all sections where you are asked to write code.</font>\n",
        "- <font color=\"red\">Carefully read the course outline for the plagiarism policy and late-day rules.</font>\n",
        "- <font color=\"red\">Run all cells before submitting to receive full credit.</font>\n",
        "- <font color=\"red\">Do not delete or modify any pre-written code.</font>\n",
        "- <font color=\"red\">Attempt every part; each section builds toward a complete understanding.</font>\n",
        "Failure to follow these rules may result in marks deduction.\n",
        "\n",
        "## Submission Instructions\n",
        "- <font color=\"red\">Rename the notebook according to you Student ID. For example if you student ID is 27010001, rename the file to 27010001_PART_A</font>\n",
        "- <font color=\"red\">Once you're done with both Part A and Part B, zip the folder containing both notebooks and rename the folder to your Student ID as well.</font>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ed1ae8",
      "metadata": {
        "id": "e2ed1ae8"
      },
      "source": [
        "## 1. Dataset: *Alice‚Äôs Adventures in Wonderland* [3 Points]\n",
        "\n",
        "In this assignment, all students will train their Byte Pair Encoding (BPE) tokenizer on the **same fixed corpus**\n",
        "\n",
        "This text is in the **public domain** and is commonly used in NLP experiments due to its clean language and rich word structure.\n",
        "\n",
        "\n",
        "### Corpus Details\n",
        "- Source: *Alice‚Äôs Adventures in Wonderland*\n",
        "- Size: **500 words**\n",
        "- Preprocessing:\n",
        "  - Lowercased\n",
        "  - Punctuation removed\n",
        "  - Words split on whitespace\n",
        "- Each word is treated as an **independent unit**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "id": "MxK2nSCse_jA",
        "outputId": "ca3632a0-98d7-4418-fa59-cc1b3f19d2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MxK2nSCse_jA",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'alice.txt', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "34b2d996",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34b2d996",
        "outputId": "60d4cf20-0054-4bb3-ee1d-d9934a7474a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 2738\n",
            "Most common words: [('the', 1623), ('and', 842), ('to', 721), ('a', 622), ('she', 537), ('it', 526), ('of', 507), ('said', 462), ('i', 399), ('alice', 385)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Load the fixed corpus\n",
        "with open(\"/content/alice.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "# Basic preprocessing\n",
        "text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "words = text.split()\n",
        "\n",
        "# Count word frequencies\n",
        "corpus = Counter(words)\n",
        "\n",
        "print(\"Number of unique words:\", len(corpus))\n",
        "print(\"Most common words:\", corpus.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "96d694dd",
      "metadata": {
        "id": "96d694dd"
      },
      "outputs": [],
      "source": [
        "def build_initial_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Build the initial BPE vocabulary from a word-frequency corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (dict): A dictionary mapping words to their frequencies.\n",
        "                       Example: {\"low\": 5, \"lower\": 2}\n",
        "\n",
        "    Returns:\n",
        "        vocab (dict): A dictionary mapping space-separated character sequences\n",
        "                      (with </w> appended) to their frequencies.\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "\n",
        "    # 1. Iterate over each word and its frequency in the corpus\n",
        "    for word, freq in corpus.items():\n",
        "        # 2. Convert the word into a list of characters\n",
        "        chars = list(word)\n",
        "\n",
        "        # 3. Append the end-of-word marker '</w>'\n",
        "        chars.append(\"</w>\")\n",
        "\n",
        "        # 4. Join the symbols with spaces to form the vocabulary key\n",
        "        key = \" \".join(chars)\n",
        "\n",
        "        # 5. Store the frequency in the vocab dictionary\n",
        "        vocab[key] = freq\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5f863b45",
      "metadata": {
        "id": "5f863b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9694ff57-ca86-417d-a03e-f287206ad8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vocabulary entries: 2738\n",
            "a l i c e </w> -> 385\n",
            "w a s </w> -> 357\n",
            "b e g i n n i n g </w> -> 13\n",
            "t o </w> -> 721\n",
            "g e t </w> -> 46\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "vocab = build_initial_vocab(corpus)\n",
        "\n",
        "print(\"Number of vocabulary entries:\", len(vocab))\n",
        "\n",
        "# Print a few example entries\n",
        "for i, (k, v) in enumerate(vocab.items()):\n",
        "    print(f\"{k} -> {v}\")\n",
        "    if i == 4:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29894b3",
      "metadata": {
        "id": "f29894b3"
      },
      "source": [
        "## 2. Computing Pair Frequencies [5 Points]\n",
        "\n",
        "The next step in Byte Pair Encoding (BPE) is to compute the **frequency of adjacent symbol pairs** across the entire vocabulary.\n",
        "\n",
        "At each iteration of BPE training:\n",
        "- We identify the **most frequent adjacent pair of symbols**\n",
        "- This pair will later be merged into a single new symbol\n",
        "\n",
        "---\n",
        "\n",
        "### What is a Symbol Pair?\n",
        "\n",
        "- Given a vocabulary entry like:\n",
        "- l o w </w>\n",
        "- The adjacent symbol pairs are:\n",
        "- (l, o), (o, w), (w, </w>)\n",
        "\n",
        "\n",
        "\n",
        "If the word occurs with frequency `f`, then **each of these pairs contributes `f`** to the overall pair frequency count.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Frequencies Matter\n",
        "\n",
        "BPE is a **frequency-based greedy algorithm**:\n",
        "- Pairs that occur most frequently are merged first\n",
        "- Frequencies must be weighted by word counts in the corpus\n",
        "\n",
        "Failing to account for frequencies correctly will result in incorrect merge behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "\n",
        "You will implement a function that:\n",
        "1. Iterates over all vocabulary entries\n",
        "2. Extracts adjacent symbol pairs\n",
        "3. Accumulates their frequencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d4532785",
      "metadata": {
        "id": "d4532785"
      },
      "outputs": [],
      "source": [
        "def get_pair_frequencies(vocab):\n",
        "    \"\"\"\n",
        "    Compute frequencies of adjacent symbol pairs in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab (dict): A dictionary mapping space-separated symbol sequences\n",
        "                      to their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        pair_freqs (dict): A dictionary mapping symbol pairs (tuples)\n",
        "                           to their total frequency.\n",
        "    \"\"\"\n",
        "    pair_freqs = {}\n",
        "\n",
        "    # 1. Iterate over each vocabulary entry and its frequency\n",
        "    for entry, freq in vocab.items():\n",
        "        # 2. Split the entry into a list of symbols\n",
        "        symbols = entry.split()\n",
        "\n",
        "        # 3. Extract all adjacent symbol pairs\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "\n",
        "            # 4. Accumulate pair frequencies weighted by word frequency\n",
        "            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
        "\n",
        "    return pair_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd88309c",
      "metadata": {
        "id": "bd88309c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095ec531-00cd-47a7-d229-847da58023c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent symbol pairs:\n",
            "('e', '</w>') -> 5716\n",
            "('h', 'e') -> 3772\n",
            "('t', 'h') -> 3476\n",
            "('t', '</w>') -> 3265\n",
            "('d', '</w>') -> 3202\n",
            "('s', '</w>') -> 2202\n",
            "('i', 'n') -> 2024\n",
            "('e', 'r') -> 1807\n",
            "('n', '</w>') -> 1786\n",
            "('a', 'n') -> 1604\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "pair_freqs = get_pair_frequencies(vocab)\n",
        "\n",
        "# Print the most frequent pairs\n",
        "sorted_pairs = sorted(pair_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 10 most frequent symbol pairs:\")\n",
        "for pair, freq in sorted_pairs[:10]:\n",
        "    print(pair, \"->\", freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec514051",
      "metadata": {
        "id": "ec514051"
      },
      "source": [
        "## 3. Merging the Most Frequent Pair [5 Points]\n",
        "\n",
        "Once we have computed the frequencies of all adjacent symbol pairs, the next step in Byte Pair Encoding (BPE) is to **merge the most frequent pair**.\n",
        "\n",
        "This merge:\n",
        "- Replaces every occurrence of the selected symbol pair\n",
        "- Creates a new combined symbol\n",
        "- Updates the vocabulary accordingly\n",
        "\n",
        "This operation is applied **globally** across the entire vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "### Example\n",
        "\n",
        "Suppose the most frequent pair is: ('l', 'o')\n",
        "- Then the symbols: l o w </w>\n",
        "- become: lo w </w>\n",
        "\n",
        "Importantly:\n",
        "- Only **adjacent** occurrences are merged\n",
        "- Other symbols remain unchanged\n",
        "- Frequencies of words remain the same\n",
        "\n",
        "### Your Task\n",
        "\n",
        "You will implement a function that:\n",
        "1. Takes a symbol pair to merge\n",
        "2. Replaces all occurrences of that pair in the vocabulary\n",
        "3. Returns an updated vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9f7d76f7",
      "metadata": {
        "id": "9f7d76f7"
      },
      "outputs": [],
      "source": [
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"\n",
        "    Merge all occurrences of a symbol pair in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        pair (tuple): A tuple of two symbols to merge, e.g. ('l', 'o')\n",
        "        vocab (dict): Current BPE vocabulary mapping symbol sequences\n",
        "                      to their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        new_vocab (dict): Updated vocabulary after merging the pair.\n",
        "    \"\"\"\n",
        "    new_vocab = {}\n",
        "    a, b = pair\n",
        "    merged_symbol = a + b\n",
        "\n",
        "    # 1. Iterate over each vocabulary entry and its frequency\n",
        "    for entry, freq in vocab.items():\n",
        "        # 2. Split the entry into a list of symbols\n",
        "        symbols = entry.split()\n",
        "\n",
        "        new_symbols = []\n",
        "        i = 0\n",
        "\n",
        "        # 3. Scan through the symbols and merge occurrences of the given pair\n",
        "        while i < len(symbols):\n",
        "            if (\n",
        "                i < len(symbols) - 1\n",
        "                and symbols[i] == a\n",
        "                and symbols[i + 1] == b\n",
        "            ):\n",
        "                # merge the pair\n",
        "                new_symbols.append(merged_symbol)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_symbols.append(symbols[i])\n",
        "                i += 1\n",
        "\n",
        "        # 4. Reconstruct the updated symbol sequence\n",
        "        new_entry = \" \".join(new_symbols)\n",
        "\n",
        "        # 5. Store it in new_vocab with the same frequency\n",
        "        new_vocab[new_entry] = new_vocab.get(new_entry, 0) + freq\n",
        "\n",
        "    return new_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fac4b050",
      "metadata": {
        "id": "fac4b050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaa1b75-561e-4811-ff8b-7c5feecc9bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent pair: ('e', '</w>')\n",
            "a l i c e</w> -> 385\n",
            "w a s </w> -> 357\n",
            "b e g i n n i n g </w> -> 13\n",
            "t o </w> -> 721\n",
            "g e t </w> -> 46\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "\n",
        "# Identify the most frequent pair\n",
        "pair_freqs = get_pair_frequencies(vocab)\n",
        "best_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "\n",
        "print(\"Most frequent pair:\", best_pair)\n",
        "\n",
        "# Apply merge\n",
        "vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "# Inspect updated vocabulary\n",
        "for i, (k, v) in enumerate(vocab.items()):\n",
        "    print(f\"{k} -> {v}\")\n",
        "    if i == 4:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c7d9cb",
      "metadata": {
        "id": "b6c7d9cb"
      },
      "source": [
        "## 4. Training the BPE Tokenizer [15 Points]\n",
        "\n",
        "So far, you have implemented the individual components of Byte Pair Encoding (BPE):\n",
        "- Initial vocabulary construction\n",
        "- Pair frequency computation\n",
        "- Merging a symbol pair\n",
        "\n",
        "In this section, you will **combine all of these steps** into a full BPE training loop that performs **multiple merge operations**.\n",
        "\n",
        "This loop is the core of the BPE algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### How BPE Training Works (Recap)\n",
        "\n",
        "For a fixed number of merge operations:\n",
        "1. Compute pair frequencies from the current vocabulary\n",
        "2. Select the **most frequent symbol pair**\n",
        "3. Merge that pair across the entire vocabulary\n",
        "4. Record the merge operation\n",
        "5. Repeat\n",
        "\n",
        "Each merge introduces a new subword token into the vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "\n",
        "You will implement a function that:\n",
        "- Trains a BPE tokenizer for a specified number of merges\n",
        "- Returns both:\n",
        "  - The final vocabulary\n",
        "  - The ordered list of merge operations\n",
        "\n",
        "The **order of merges matters** and must be preserved.\n",
        "\n",
        "---\n",
        "Important Notes\n",
        "\n",
        "If no pairs are available to merge, you may stop early\n",
        "\n",
        "In case of ties, Python‚Äôs default behavior is acceptable\n",
        "\n",
        "Do not shuffle or reorder merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6e5b0e80",
      "metadata": {
        "id": "6e5b0e80"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_bpe(corpus, num_merges):\n",
        "    \"\"\"\n",
        "    Train a Byte Pair Encoding (BPE) tokenizer.\n",
        "\n",
        "    Args:\n",
        "        corpus (dict): Word-frequency corpus.\n",
        "        num_merges (int): Number of BPE merge operations to perform.\n",
        "\n",
        "    Returns:\n",
        "        vocab (dict): Final BPE vocabulary after all merges.\n",
        "        merges (list): List of merged symbol pairs, in order.\n",
        "    \"\"\"\n",
        "    # Step 1: Build the initial vocabulary\n",
        "    vocab = build_initial_vocab(corpus)\n",
        "\n",
        "    merges = []\n",
        "\n",
        "    # Step 2: Perform BPE merges\n",
        "    for _ in range(num_merges):\n",
        "        # 1. Compute pair frequencies\n",
        "        pair_freqs = get_pair_frequencies(vocab)\n",
        "\n",
        "        # Stop early if no pairs remain\n",
        "        if not pair_freqs:\n",
        "            break\n",
        "\n",
        "        # 2. Find the most frequent symbol pair\n",
        "        best_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "\n",
        "        # 3. Merge that pair in the vocabulary\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "        # 4. Record the merge operation\n",
        "        merges.append(best_pair)\n",
        "\n",
        "        # 5. Updated vocab is used in next iteration automatically\n",
        "\n",
        "    return vocab, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5b8f64da",
      "metadata": {
        "id": "5b8f64da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96510d4-569d-4e86-d766-e7856583d63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merges: [('e', 's'), ('es', 't'), ('est', '</w>'), ('l', 'o'), ('lo', 'w')]\n",
            "Sample vocab:\n",
            "low</w> -> 5\n",
            "low e r </w> -> 2\n",
            "newest</w> -> 6\n",
            "wi d est</w> -> 3\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "\n",
        "toy = {\"low\": 5, \"lower\": 2, \"newest\": 6, \"widest\": 3}\n",
        "vocab, merges = train_bpe(toy, num_merges=10)\n",
        "\n",
        "print(\"Merges:\", merges[:5])\n",
        "print(\"Sample vocab:\")\n",
        "for i, (k, v) in enumerate(vocab.items()):\n",
        "    print(k, \"->\", v)\n",
        "    if i == 4:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48aea09e",
      "metadata": {
        "id": "48aea09e"
      },
      "source": [
        "## Theoretical Question: Interpreting Early BPE Merges [4 Points]\n",
        "\n",
        "Using the output of your BPE training:\n",
        "\n",
        "### Question\n",
        "Identify **one** of the **first 10 merge operations** learned by your BPE algorithm.\n",
        "\n",
        "Your answer must include:\n",
        "\n",
        "1. **The merge operation**  \n",
        "   - Write the merged token explicitly\n",
        "\n",
        "2. **Explanation of frequency**  \n",
        "   - Explain *why* this merge is frequent in the given corpus  \n",
        "\n",
        "3. **Corpus evidence**  \n",
        "   - Reference **at least two words** from the corpus in which this merge appears  \n",
        "\n",
        "### Notes\n",
        "- You may use intermediate printouts from your code to support your explanation\n",
        "- Answers that only list the merge without explanation will not receive full credit\n",
        "- Be concise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Merge operation:**  \n",
        "('l', 'o') =`lo`\n",
        "\n",
        "**Why this merge is frequent:**  \n",
        "The letters **l** and **o** often appear next to each other in the text</b>\n",
        "Because BPE always merges the most common adjacent letters first, this pair is learned early\n",
        "\n",
        "**Corpus evidence:**  \n",
        "This merged token appears in multiple words from the corpus, such as:\n",
        "- **low**\n",
        "- **lower**\n",
        "- **lowest**\n",
        "\n",
        "For example, `l o w </w>` becomes `lo w </w>` after the merge.\n",
        "\n",
        "\n",
        "The merge `('l', 'o')=lo` is learned early because it occurs many times in common words, making it a useful and frequent pattern in the corpus."
      ],
      "metadata": {
        "id": "f7iYIn8EkFt2"
      },
      "id": "f7iYIn8EkFt2"
    },
    {
      "cell_type": "markdown",
      "id": "1f9a214c",
      "metadata": {
        "id": "1f9a214c"
      },
      "source": [
        "## 5. Tokenizing New Text with Learned BPE Merges [4 Points]\n",
        "\n",
        "Training BPE produces an **ordered list of merge operations**.  \n",
        "To tokenize new text, we apply these merges (in order) to each word.\n",
        "\n",
        "This simulates how BPE tokenizers are used in practice:\n",
        "- Start with characters + end-of-word marker\n",
        "- Apply merges sequentially\n",
        "- Output the resulting subword tokens\n",
        "\n",
        "---\n",
        "\n",
        "### Key Idea: Merge Order Matters\n",
        "\n",
        "If the merge operations are:\n",
        "- ('l', 'o') -> 'lo'\n",
        "- ('lo', 'w') -> 'low'\n",
        "\n",
        "\n",
        "Then tokenizing `\"low\"` must apply these merges in the same order to recover:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "\n",
        "You will implement a function that tokenizes a single word using the learned merge rules.\n",
        "\n",
        "Steps:\n",
        "1. Convert the word into a list of characters\n",
        "2. Append `</w>`\n",
        "3. For each learned merge pair:\n",
        "   - Replace all adjacent occurrences of that pair with the merged symbol\n",
        "4. Return the final list of symbols\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c934b995",
      "metadata": {
        "id": "c934b995"
      },
      "outputs": [],
      "source": [
        "def tokenize_word_bpe(word, merges):\n",
        "    \"\"\"\n",
        "    Tokenize a single word using a learned list of BPE merges.\n",
        "\n",
        "    Args:\n",
        "        word (str): Input word (already lowercased, no punctuation).\n",
        "        merges (list): List of merge operations (tuples), in order.\n",
        "\n",
        "    Returns:\n",
        "        tokens (list): List of BPE tokens after applying merges.\n",
        "    \"\"\"\n",
        "    # Step 1: start from characters + end-of-word marker\n",
        "    tokens = list(word) + [\"</w>\"]\n",
        "\n",
        "    # Step 2: apply merges in order\n",
        "    for a, b in merges:\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(tokens):\n",
        "            # If we find the merge pair (a, b), merge them\n",
        "            if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
        "                new_tokens.append(a + b)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "\n",
        "        tokens = new_tokens\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a2f92837",
      "metadata": {
        "id": "a2f92837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce030efa-31c6-49b1-d68d-ac47119d01c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice -> ['alice</w>']\n",
            "wonderland -> ['w', 'on', 'd', 'er', 'l', 'and</w>']\n",
            "rabbit -> ['r', 'a', 'b', 'b', 'it</w>']\n",
            "curious -> ['c', 'u', 'r', 'i', 'ou', 's</w>']\n",
            "unseenword -> ['u', 'n', 's', 'e', 'en', 'w', 'or', 'd</w>']\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "\n",
        "# Train a small BPE model\n",
        "vocab, merges = train_bpe(corpus, num_merges=50)\n",
        "\n",
        "test_words = [\"alice\", \"wonderland\", \"rabbit\", \"curious\", \"unseenword\"]\n",
        "\n",
        "for w in test_words:\n",
        "    print(w, \"->\", tokenize_word_bpe(w, merges))\n",
        "\n",
        "# print(merges) -> maybe this will help with the theoratical questions. hmmmm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac66a07",
      "metadata": {
        "id": "5ac66a07"
      },
      "source": [
        "## Theoretical Question: Frequent but Linguistically Unintuitive Merges [5 Points]\n",
        "\n",
        "### Question\n",
        "Choose **one merge operation** from your learned BPE merges that is frequent but linguistically unintuitive.\n",
        "\n",
        "Your answer must include:\n",
        "\n",
        "1. **The merge operation**  \n",
        "   - Write the merge explicitly\n",
        "\n",
        "2. **Why BPE learns this merge**  \n",
        "   - Explain why this merge has high frequency in the corpus  \n",
        "\n",
        "3. **Impact on tokenization**  \n",
        "   - State whether this merge **helps or hurts** downstream tokenization. Justify your answer (briefly)\n",
        "\n",
        "### Notes\n",
        "- The merge must be taken from your learned merge list\n",
        "- Focus on reasoning rather than linguistic terminology\n",
        "- Answers should be grounded in your observed merge statistics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Merge operation:**  \n",
        "`('e', '</w>')  e</w>`\n",
        "\n",
        "**Why BPE learns this merge:**  \n",
        "BPE learns this merge because it looks only at how often letter pairs appear together.  \n",
        "In the corpus, many words end with the letter **e**, so the pair **e </w>** appears very often.\n",
        "\n",
        "From the training output, this pair had the **highest frequency**, so BPE merged it early.\n",
        "\n",
        "**Examples from the corpus:**  \n",
        "This merge appears at the end of many common words, such as:\n",
        "- **the**\n",
        "- **she**\n",
        "- **alice**\n",
        "\n",
        "Each of these words ends with `e`, followed by the end-of-word marker `</w>`.\n",
        "\n",
        "**Impact on tokenization:**  \n",
        "This merge mostly **hurts or gives little benefit**.\n",
        "\n",
        "**Reason:**  \n",
        "- The token `e</w>` does not represent a meaningful part of a word.\n",
        "- It is only frequent because of word endings, not because it carries meaning.\n",
        "- It uses up a merge operation that could have been used for more useful patterns.\n",
        "\n",
        "**Conclusion:**  \n",
        "The merge `('e', '</w>')  e</w>` is frequent due to many words ending in `e`, but it is linguistically unintuitive and not very helpful for good tokenization."
      ],
      "metadata": {
        "id": "qjEqZOF_mbXW"
      },
      "id": "qjEqZOF_mbXW"
    },
    {
      "cell_type": "markdown",
      "id": "e5a96936",
      "metadata": {
        "id": "e5a96936"
      },
      "source": [
        "## 6. Tokenizing Full Text with BPE [5 Points]\n",
        "\n",
        "So far, you have implemented BPE tokenization for a **single word**.  \n",
        "In practice, tokenizers are applied to **entire texts**, usually by tokenizing each word independently and then concatenating the results.\n",
        "\n",
        "In this section, you will use your existing `tokenize_word_bpe` function to tokenize a full input string.\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "\n",
        "You will implement a helper function that:\n",
        "1. Splits the input text on whitespace\n",
        "2. Applies BPE tokenization to each word\n",
        "3. Returns a flattened list of BPE tokens\n",
        "\n",
        "This mirrors how BPE-based tokenizers are typically used in real NLP pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8d102f2e",
      "metadata": {
        "id": "8d102f2e"
      },
      "outputs": [],
      "source": [
        "def tokenize_text_bpe(text, merges):\n",
        "    \"\"\"\n",
        "    Tokenize a whitespace-separated string using BPE merges.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text (already lowercased/cleaned).\n",
        "        merges (list): Learned BPE merges (tuples), in order.\n",
        "\n",
        "    Returns:\n",
        "        tokens (list): Flattened list of BPE tokens for the full text.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "\n",
        "    # 1. Split text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # 2. Tokenize each word and collect tokens\n",
        "    for word in words:\n",
        "        word_tokens = tokenize_word_bpe(word, merges)\n",
        "        tokens.extend(word_tokens)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "537433f2",
      "metadata": {
        "id": "537433f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8023f5da-081a-4c09-cb64-78e0852cdeb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alice</w>', 'w', 'as</w>', 'b', 'e', 'g', 'in', 'n', 'ing</w>', 'to</w>', 'g', 'e', 't</w>', 'v', 'er', 'y</w>', 't', 'i', 'r', 'ed</w>']\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CODE, IT IS FOR GRADING.\n",
        "sample = \"alice was beginning to get very tired\"\n",
        "tokens = tokenize_text_bpe(sample.lower(), merges)\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bba5422",
      "metadata": {
        "id": "3bba5422"
      },
      "source": [
        "## Theoretical Question: Limitations of BPE Tokenization [4 Points]\n",
        "\n",
        "### Question\n",
        "Provide **one example word** that is **poorly tokenized** after training your BPE tokenizer.\n",
        "\n",
        "Your answer must include:\n",
        "\n",
        "1. **Example word**  \n",
        "   - The word must be taken from the Alice corpus\n",
        "\n",
        "2. **Explanation**  \n",
        "   - Explain which merge operations led to this tokenization  \n",
        "   - Describe why the resulting tokenization may be considered suboptimal or undesirable\n",
        "\n",
        "\n",
        "### Notes\n",
        "- Answers must reference observed behavior from your tokenizer\n",
        "- Generic answers will not receive full credit\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0f60889",
      "metadata": {
        "id": "f0f60889"
      },
      "source": [
        "\n",
        "\n",
        "**Example word:**  \n",
        "`beginning`\n",
        "\n",
        "**Observed tokenization:**  \n",
        "`b e g in n ing</w>`\n",
        "\n",
        "**Explanation:**  \n",
        "During training, BPE learned very common letter patterns such as:\n",
        "- `('i', 'n') in`\n",
        "- `('i', 'n', 'g') ing</w>`\n",
        "\n",
        "These patterns appear in many different words, so they were merged early  \n",
        "However, the full word **beginning** is not very frequent in the corpus, so BPE did not learn merges that combine the whole word or larger parts like `begin`.\n",
        "\n",
        "**Why this tokenization is suboptimal:**  \n",
        "The word is split into many small pieces, which increases the number of tokens.  \n",
        "This makes the tokenization less efficient and less intuitive compared to a split like `begin + ning</w>` or a single token.\n",
        "\n",
        "**Conclusion:**  \n",
        "This shows a limitation of BPE: it merges frequent patterns globally, but this can lead to poor tokenization for some individual words"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pe10gYe0ljRJ"
      },
      "id": "Pe10gYe0ljRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative AI Usage Report\n",
        "\n",
        "This assignment was completed with the assistance of a generative AI tool, in accordance with the course AI policy.\n",
        "\n",
        "### AI Tool Used\n",
        "- **Tool:** ChatGPT 5.2 ‚Äì Thinking  \n",
        "- **Provider:** OpenAI  \n",
        "- **Account Type:** Plus Account  \n",
        "\n",
        "---\n",
        "\n",
        "### Purpose of AI Use\n",
        "The AI tool was used as a **learning and support aid**, not as a direct replacement for independent work. Specifically, it was used to:\n",
        "- Understand the **Byte Pair Encoding (BPE)** algorithm conceptually\n",
        "- Receive guidance on implementing BPE components from scratch in Python\n",
        "- Clarify theoretical questions related to BPE behavior and limitations\n",
        "- Improve clarity and structure of written theoretical explanations\n",
        "\n",
        "---\n",
        "\n",
        "### Prompts Used (Examples)\n",
        "- ‚ÄúExplain what this BPE function does step by step.‚Äù\n",
        "- ‚ÄúHelp implement BPE merge and pair frequency functions from scratch.‚Äù\n",
        "- ‚ÄúExplain why this BPE merge is frequent but linguistically unintuitive.‚Äù\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### AI Outputs Used\n",
        "- Explanations of BPE concepts (pair frequencies, merges, tokenization)\n",
        "- Example implementations of BPE-related functions (adapted to match assignment constraints)\n",
        "- Draft explanations for theoretical questions, later refined by the student\n",
        "\n",
        "---\n",
        "\n",
        "### Edits and Student Contributions\n",
        "- All AI-generated code was **reviewed, tested, and adapted** by the student.\n",
        "- Variable names, formatting, and logic were adjusted to match assignment requirements.\n",
        "- Theoretical answers were **rewritten in simpler wording** and aligned with the student‚Äôs actual outputs.\n",
        "- Final responses reflect the student‚Äôs understanding and interpretation of results.\n",
        "\n",
        "---\n",
        "\n",
        "### Academic Integrity Statement\n",
        "The student confirms that:\n",
        "- No prohibited libraries or external BPE implementations were used.\n",
        "- All submitted work was reviewed and understood by the student.\n",
        "- AI assistance was used transparently as a support tool, not to bypass learning objectives.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Citation\n",
        "ChatGPT 5.2 ‚Äì Thinking (2026).  \n",
        "Prompt: ‚ÄúExplain why this BPE merge is frequent but linguistically unintuitive.‚Äù  \n",
        "Response: Explanation of frequency-based merges and their impact on tokenization.  \n",
        "OpenAI. Plus Account."
      ],
      "metadata": {
        "id": "bt-UZanzoBMW"
      },
      "id": "bt-UZanzoBMW"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlzGAaSXoBsB"
      },
      "id": "zlzGAaSXoBsB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}